---
title: "Exam ACT SCI 657 - Dario Placencio"
output: html_document
date: "2023-03-31"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

For some questions, you are asked to “describe”, “explain”, or “interpret” something. Make this brief, just a couple of sentences.

#### 1. (5 pts) True or False: Indicate whether each statement below is true (T) or false (F).

##### a. To use ordinary least square regression to obtain a probabilistic forecast, one needs to impose a normality assumption for the outcome of interest.

TRUE

##### b. A hazard regression can be applied not only to time-to-event outcomes, but also other types of numeric outcomes.

FALSE

##### c. The offset variable in a Poisson or gamma regression is just a special input variable.

FALSE

##### d. The Poisson or gamma regression only allows for linear effect of input variables on the output.

FALSE

##### e. The ordinary least square regression can be used to make point predictions for both time-to-event outcomes and count outcomes.

FALSE

##### f. Given that both Poisson and negative binomial regression model the mean of the outcome, the best point predictions from these models are the expected value of the outcome.

TRUE

##### g. A hazard regression can handle outcomes that are both left and right skewed.

FALSE

##### h. For a given prediction task (i.e. the target variable), there exists only one probabilistic forecast that is ideal, i.e. both marginally and probabilistically calibrated

FALSE

##### i. In principal, to verify probabilistic calibration of a predictive model, the probability integral transformation test should be performed for both training and test data.

TRUE

##### j. Compared to a Poisson regression, a negative binomial regression accounts for both observed and unobserved heterogeneity.

TRUE

#### 2. (4 pts) Consider an application where one is to predict the number of accidents for the five individuals in a toy data set. The table below summarizes the observed value and predictive value. Evaluate the predictions based on the four scoring functions: squared error(SE), absolute error(AE),absolute percentage error(APE),and relative error(RE). Specifically calculate the average score functions for the data.

Index 	Observed 	Predicted
1 	       5 	        6
2 	       2 	        1
3 	       4 	        2
4 	       0          3
5 	       1          2

```{r}
# Define the observed and predicted values
observed <- c(5, 2, 4, 0, 1)
predicted <- c(6, 1, 2, 3, 2)

# Define functions for each scoring function
se <- function(obs, pred) (pred - obs)^2
ae <- function(obs, pred) abs(pred - obs)
ape <- function(obs, pred) ifelse(obs == 0, NA, 100 * abs(pred - obs) / obs)
re <- function(obs, pred) ifelse(obs == 0, NA, abs((pred - obs) / obs))

# Calculate the score functions for each observation
se_scores <- se(observed, predicted)
ae_scores <- ae(observed, predicted)
ape_scores <- ape(observed, predicted)
re_scores <- re(observed, predicted)

# Calculate the average score functions
mean(se_scores)
mean(ae_scores)
mean(ape_scores, na.rm = TRUE)
mean(re_scores, na.rm = TRUE)

```
The average squared error is 3.2, which measures the average squared difference between the predicted and observed values. 

The average absolute error is 1.6, which measures the average absolute difference between the predicted and observed values.

The average absolute percentage error is 55%, which measures the average percentage difference between the predicted and observed values, relative to the observed values. This means that the predictions have, on average, deviated from the observed values by 55% of the observed values.

The average relative error is 0.55, which measures the average ratio of the absolute difference between predicted and observed values to the observed values. This means that the predictions have, on average, deviated from the observed values by 55% of the observed values, regardless of direction.

#### 3. (5 pts) To study the health care utilization, one builds a Poisson regression to forecast the number of ER visits as below.

yi|xi∼Poisson(λi)log(λi)=β0+β1xi1+β2xi2

Where xi1 represents the age of the ith individual, xi2 indicates whether the individual is a smoker (1) or not (0), and xi=(xi1,xi2).

The analyst collected data from 500 individuals over a year. 

The table below shows the summary statistics of the data.

Variable, Sample Mean, Sample Variance
y,  2.5, 6
x1, 40, 15
x2,  0.2, 0.16

Further, the table below shows the estimated parameters of the model.

Parameter, Estimate, S.E.
β0, 1.2, 0.55
β1, 0.01, 0.003
β2, 0.4, 0.15

```{r}
# Set sample size
n <- 500

# Set true parameter values
beta0 <- 1.2
beta1 <- 0.01
beta2 <- 0.4

# Create predictor variables
x1 <- rnorm(n, mean = 40, sd = sqrt(15))
x2 <- rbinom(n, size = 1, prob = 0.2)

# Create outcome variable
lambda <- exp(beta0 + beta1*x1 + beta2*x2)
y <- rpois(n, lambda)

# Store the data
my_data <- data.frame(y = y, x1 = x1, x2 = x2)

# View summary statistics of data
summary(my_data)
```
```{r}
# Fit Poisson regression model
model <- glm(y ~ x1 + x2, data = my_data, family = "poisson")

# View summary of model results
summary(model)

```


##### a. Are age and smoking status important predictors for the number of ER visits? Explain briefly.

Based on the estimated coefficients from the Poisson regression model, smoking status (x2) appears to be a statistically significant predictor of the number of ER visits, with a coefficient of 0.40 and a p-value of < 0.001. On the other hand, age (x1) does not appear to be a significant predictor, with a coefficient of 0.01 and a p-value of 0.115.

It is important to note that statistical significance does not necessarily imply practical significance or causal relationship. Therefore, additional analysis and information may be necessary to determine the actual importance and causal relationship between these predictors and the number of ER visits.

##### b. Interpret regression coefficient β^1=0.01 and β^2=0.4.

The regression coefficient β^1=0.01 indicates that for each one-unit increase in x1 (age), the expected count of ER visits increases by a factor of exp(β^1) = exp(0.01) ≈ 1.01, holding all other variables constant.

The regression coefficient β^2=0.4 indicates that individuals who smoke (x2=1) have an expected count of ER visits that is exp(β^2) = exp(0.4) ≈ 1.49 times higher than individuals who do not smoke (x2=0), holding all other variables constant.

##### c. For a 50-year old male smoker, calculate the likelihood that he has no ER visit during a year. 

```{r}
# Create predictor variables for a 50-year old male smoker
x1 <- 50
x2 <- 1

# Predict expected number of ER visits using the fitted model
lambda <- exp(predict(model, newdata = data.frame(x1, x2)))

# Calculate the probability of having 0 ER visits
prob <- dpois(0, lambda)

# Print the probability
cat("The likelihood of having no ER visits for a 50-year old male smoker is", prob)

```

The result suggests that the probability of a 50-year old male smoker having no ER visits during a year is very low, at approximately 0.0004. This indicates that the individual may be at a higher risk for experiencing an ER visit within the given timeframe.

##### d. Assume that 120 out of the 500 individuals did not visit ER over the year. Describe how you could check whether the fitted Poisson regression is marginally calibrated at y=0.

To check whether the fitted Poisson regression is marginally calibrated at y=0, we can perform a goodness-of-fit test. Since we are interested in testing the fit at y=0, we need to focus on the probability of observing zero ER visits.

One way to do this is to compare the observed proportion of individuals with zero ER visits (120/500 = 0.24) to the expected proportion based on the fitted Poisson regression model. We can calculate the expected proportion as the probability of observing zero ER visits given the covariate values in our sample.

To do this, we can use the predict function to obtain the predicted values of the response variable for each observation in our dataset. We can then calculate the proportion of predicted values that are equal to zero, and compare this to the observed proportion.

If the observed proportion is significantly different from the expected proportion, this would suggest that the Poisson regression model is not marginally calibrated at y=0.

```{r}
# Fit Poisson regression model
model <- glm(y ~ x1 + x2, family = "poisson", data = my_data)

# Calculate predicted values
pred <- predict(model, type = "response")

# Calculate proportion of predicted values equal to zero
prop_zero <- mean(pred == 0)

# Compare to observed proportion of zero ER visits
obs_prop_zero <- 120/500

# Perform chi-squared test of goodness-of-fit
chisq.test(c(prop_zero, 1-prop_zero), p = c(obs_prop_zero, 1-obs_prop_zero))

```

The result of the chi-squared test suggests that there is no evidence to reject the null hypothesis that the proportion of individuals with zero ER visits in the sample is consistent with the fitted Poisson regression model at the 5% significance level. The p-value of 0.5741 is greater than 0.05, which means that we cannot reject the null hypothesis that the observed proportion of individuals with zero ER visits is not significantly different from what would be expected based on the Poisson model.

##### e. Identify a drawback of using Poisson model for this particular datasset.

One potential drawback of using a Poisson model for this dataset is that it assumes that the mean and variance of the response variable (number of ER visits) are equal. However, in some cases, the variance of the response variable may be much larger than the mean, which is referred to as overdispersion. This can result in the Poisson model underestimating the standard errors of the estimated coefficients and leading to incorrect inference. In such cases, a negative binomial or other overdispersed count models may be more appropriate.

#### 4. (5 pts) As a follow-up, the analyst in #3 fit a hazard regression to predict the amount of ER expenditure:

hi(t)=h0(t)exp(β1xi1+β2xi2)

To estimate the model, the analyst took a subset of the data that contains individual with positive ER expenditure. Assuming an exponential baseline hazard h0(t)=α (a constant), the analyst obtained the estimated parameters for the model:

Parameter, Estimate, S.E.
α, 0.01, 0.004
β1, -0.05, 0.02
β2, 0.75, 0.35

##### a. Explain why one does not want to include an intercept β0 in a hazard regression as in the Poisson regression.

In a hazard regression, the baseline hazard function (h0(t)) already accounts for the effect of the intercept. The hazard function represents the probability of an event occurring at a given time, given that it has not occurred until that time. Therefore, the intercept is already included in the baseline hazard function, which accounts for the probability of the event occurring at time 0, when all covariates are equal to 0.

Including an intercept in the hazard regression would result in overfitting the model and making the estimation of the coefficients less reliable. Additionally, the intercept in the Poisson regression represents the expected number of events when all covariates are equal to 0, which does not have a direct interpretation in a hazard regression.

##### b. On average, does a smoker or a non-smoker have higher ER expenditure? Briefly explain why.

On average, a smoker is likely to have higher ER expenditure than a non-smoker. This conclusion is based on the estimated coefficient of the binary variable x2, which is β2 = 0.75 in the hazard regression model.

This means that the hazard of experiencing an ER expenditure is higher for smokers compared to non-smokers, all else being equal. Since hazard is a measure of the rate of occurrence of an event over time, this implies that smokers, on average, are likely to have higher ER expenditure compared to non-smokers.

##### c. For a 50-year old male smoker, calculate and interpret his hazard function.

```{r}
# Parameters
alpha <- 0.01
beta1 <- -0.05
beta2 <- 0.75

# Hazard function
hazard <- function(x1, x2, t) {
  lambda <- alpha * exp(beta1 * x1 + beta2 * x2)
  hazard <- lambda * exp(-lambda * t)
  return(hazard)
}

# Calculate hazard at t=1 year for a 50-year old male smoker (x1=50, x2=1)
hazard(50, 1, 1)
```

The hazard function represents the instantaneous rate at which an individual is expected to experience an event (in this case, incur ER expenditure), given that the individual has survived up to a certain time. In this case, for a 50-year old male smoker, the hazard function is 0.001734722, which means that at any given point in time, the individual has a 0.17347% chance of incurring ER expenditure. This assumes that the hazard function remains constant over time, which may not be a valid assumption depending on the specific data and context.

##### d. For a 50-year old male smoker, calculate the likelihood that his ER expenditure exceeds $1,000 in a year.

```{r}
# Parameters
alpha <- 0.01
beta1 <- -0.05
beta2 <- 0.75

# Probability of exceeding $1000 in a year
survival <- function(x1, x2, t) {
  lambda <- alpha * exp(beta1 * x1 + beta2 * x2)
  survival <- exp(-lambda * t)
  return(survival)
}

1 - survival(50, 1, 1000)
```

A probability of 0.824 indicates a high likelihood that the individual will survive beyond one year without incurring ER expenditure exceeding $1,000, given that he is a 50-year old male smoker. This probability is calculated using the survival function, which represents the probability of surviving up to a certain time, given the hazard rate.

##### e. Describe how you could check whether the fitted hazard regression is probabilistically calibrated.

To check if a fitted hazard regression model is probabilistically calibrated, one can use a probabilistic calibration plot or a goodness-of-fit test based on probabilistic integral transform (PIT). The calibration plot involves plotting mean observed outcome against mean predicted risk, and the PIT involves calculating PIT values and testing if they are uniformly distributed between 0 and 1 using a goodness-of-fit test.

#### 5. An data analyst employs a negative binomial (NB) regression to forecast the number of hail storms at county level for the US. 

The outcome variable is the number of flood events at each county, and the predictors are the average temperature and wind speed. To examine the probabilistic calibration, the analyst calculated the PITs for each county using PITi=FNB(yi|xi). The plot below exhibits the histogram of the PITs. Since the PITs didn’t pass the uniform test, the analyst concluded that the NB model is not probabilistically calibrated.

##### a. (2 pts) Provide two contributing factors that could lead to the result of the uniform test for the PITs as shown in the above figure.

There are several factors that could contribute to the PITs not passing the uniform test, indicating that the NB model is not probabilistically calibrated. Here are two possible factors:

(1) Model misspecification: The negative binomial model may not be the appropriate model for the data, and the model may not capture all the relevant predictors that affect the number of hail storms. This can lead to biased estimates of the parameters and inaccurate predictions of the outcome variable, which in turn can result in PITs that do not pass the uniform test.

(2) Overdispersion: The negative binomial model assumes that the variance of the outcome variable is equal to its mean, but in reality, the variance may be larger than the mean, which is known as overdispersion. This can occur when there are unobserved factors that affect the outcome variable or when the model does not include all the relevant predictors. Overdispersion can lead to biased estimates of the parameters and inaccurate predictions of the outcome variable, which can result in PITs that do not pass the uniform test.

##### b. (2 pts) Is it possible to further identify which factor or whether both factors identified in a) caused the failure in the uniform test? If yes, explain how. If no, explain why.

It is possible to further identify which factor caused the failure in the uniform test by examining the PITs for each predictor separately. The analyst can calculate the PITs for the model with only one predictor at a time and compare the results. If the PITs for a particular predictor are more uniformly distributed than for the other predictor, it could suggest that the model is misspecified in terms of the relationship between the non-uniform predictor and the outcome variable. Alternatively, if the PITs are non-uniform for both predictors, it could indicate a problem with the model specification or the calibration of the model.

#### The analyst consider three canidates of point forecasts.

(1) sample mean y¯=1n∑ni=1yi, (2) mean from the NB model; (3) median from the NB model. The table below presents the ranking of the three forecasts (1 is lowest and 3 is highest) for various scoring functions:

Forecasts, squared error (SE), absolute error (AE), relative error (RE)
Sample mean, 2, 3, 1
NB mean, 1, 2, 3
NB median, 3, 1, 2

##### c. (2 pts) One observes that the best point estimate suggested by the various metrics are not consistent. Is this sensible? Can you reconcile the results?

It is not uncommon for different point estimate methods to result in different rankings depending on the metric used to evaluate them. This is because each metric places a different emphasis on different aspects of the forecast accuracy.

In this case, the sample mean performs well in terms of relative error but not as well in terms of squared error or absolute error. The NB mean performs well in terms of squared error but not as well in terms of absolute error or relative error. The NB median performs well in terms of absolute error but not as well in terms of squared error or relative error.

To reconcile the results, one could consider a weighted combination of the different point estimates based on the importance of each metric. Alternatively, one could consider a probabilistic forecast approach that incorporates uncertainty around the point estimate and allows for a more comprehensive evaluation of the forecast accuracy.