{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Class 6 - Dario Placencio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import nltk\n",
    "# normalization\n",
    "import unicodedata\n",
    "# stopwords\n",
    "from nltk.corpus import stopwords\n",
    "# punctuation\n",
    "import string\n",
    "# clean using regex expression\n",
    "import re\n",
    "#stemming\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "# lemmatize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "sw = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define stemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ClaimNumber</th>\n",
       "      <th>DateTimeOfAccident</th>\n",
       "      <th>DateReported</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>MaritalStatus</th>\n",
       "      <th>DependentChildren</th>\n",
       "      <th>DependentsOther</th>\n",
       "      <th>WeeklyWages</th>\n",
       "      <th>PartTimeFullTime</th>\n",
       "      <th>HoursWorkedPerWeek</th>\n",
       "      <th>DaysWorkedPerWeek</th>\n",
       "      <th>ClaimDescription</th>\n",
       "      <th>InitialIncurredCalimsCost</th>\n",
       "      <th>UltimateIncurredClaimCost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WC8285054</td>\n",
       "      <td>2002-04-09T07:00:00Z</td>\n",
       "      <td>2002-07-05T00:00:00Z</td>\n",
       "      <td>48</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>500.00</td>\n",
       "      <td>F</td>\n",
       "      <td>38.0</td>\n",
       "      <td>5</td>\n",
       "      <td>LIFTING TYRE INJURY TO RIGHT ARM AND WRIST INJURY</td>\n",
       "      <td>1500</td>\n",
       "      <td>4748.203388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WC6982224</td>\n",
       "      <td>1999-01-07T11:00:00Z</td>\n",
       "      <td>1999-01-20T00:00:00Z</td>\n",
       "      <td>43</td>\n",
       "      <td>F</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>509.34</td>\n",
       "      <td>F</td>\n",
       "      <td>37.5</td>\n",
       "      <td>5</td>\n",
       "      <td>STEPPED AROUND CRATES AND TRUCK TRAY FRACTURE ...</td>\n",
       "      <td>5500</td>\n",
       "      <td>6326.285819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WC5481426</td>\n",
       "      <td>1996-03-25T00:00:00Z</td>\n",
       "      <td>1996-04-14T00:00:00Z</td>\n",
       "      <td>30</td>\n",
       "      <td>M</td>\n",
       "      <td>U</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>709.10</td>\n",
       "      <td>F</td>\n",
       "      <td>38.0</td>\n",
       "      <td>5</td>\n",
       "      <td>CUT ON SHARP EDGE CUT LEFT THUMB</td>\n",
       "      <td>1700</td>\n",
       "      <td>2293.949087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WC9775968</td>\n",
       "      <td>2005-06-22T13:00:00Z</td>\n",
       "      <td>2005-07-22T00:00:00Z</td>\n",
       "      <td>41</td>\n",
       "      <td>M</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>555.46</td>\n",
       "      <td>F</td>\n",
       "      <td>38.0</td>\n",
       "      <td>5</td>\n",
       "      <td>DIGGING LOWER BACK LOWER BACK STRAIN</td>\n",
       "      <td>15000</td>\n",
       "      <td>17786.487170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WC2634037</td>\n",
       "      <td>1990-08-29T08:00:00Z</td>\n",
       "      <td>1990-09-27T00:00:00Z</td>\n",
       "      <td>36</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>377.10</td>\n",
       "      <td>F</td>\n",
       "      <td>38.0</td>\n",
       "      <td>5</td>\n",
       "      <td>REACHING ABOVE SHOULDER LEVEL ACUTE MUSCLE STR...</td>\n",
       "      <td>2800</td>\n",
       "      <td>4014.002925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ClaimNumber    DateTimeOfAccident          DateReported  Age Gender  \\\n",
       "0   WC8285054  2002-04-09T07:00:00Z  2002-07-05T00:00:00Z   48      M   \n",
       "1   WC6982224  1999-01-07T11:00:00Z  1999-01-20T00:00:00Z   43      F   \n",
       "2   WC5481426  1996-03-25T00:00:00Z  1996-04-14T00:00:00Z   30      M   \n",
       "3   WC9775968  2005-06-22T13:00:00Z  2005-07-22T00:00:00Z   41      M   \n",
       "4   WC2634037  1990-08-29T08:00:00Z  1990-09-27T00:00:00Z   36      M   \n",
       "\n",
       "  MaritalStatus  DependentChildren  DependentsOther  WeeklyWages  \\\n",
       "0             M                  0                0       500.00   \n",
       "1             M                  0                0       509.34   \n",
       "2             U                  0                0       709.10   \n",
       "3             S                  0                0       555.46   \n",
       "4             M                  0                0       377.10   \n",
       "\n",
       "  PartTimeFullTime  HoursWorkedPerWeek  DaysWorkedPerWeek  \\\n",
       "0                F                38.0                  5   \n",
       "1                F                37.5                  5   \n",
       "2                F                38.0                  5   \n",
       "3                F                38.0                  5   \n",
       "4                F                38.0                  5   \n",
       "\n",
       "                                    ClaimDescription  \\\n",
       "0  LIFTING TYRE INJURY TO RIGHT ARM AND WRIST INJURY   \n",
       "1  STEPPED AROUND CRATES AND TRUCK TRAY FRACTURE ...   \n",
       "2                   CUT ON SHARP EDGE CUT LEFT THUMB   \n",
       "3               DIGGING LOWER BACK LOWER BACK STRAIN   \n",
       "4  REACHING ABOVE SHOULDER LEVEL ACUTE MUSCLE STR...   \n",
       "\n",
       "   InitialIncurredCalimsCost  UltimateIncurredClaimCost  \n",
       "0                       1500                4748.203388  \n",
       "1                       5500                6326.285819  \n",
       "2                       1700                2293.949087  \n",
       "3                      15000               17786.487170  \n",
       "4                       2800                4014.002925  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the workers compensation data set\n",
    "\n",
    "dat = pd.read_csv(\"WorkersComp.csv\") \n",
    "dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words from Claim Description  \n",
    "dat['ClaimDescription'] = dat['ClaimDescription'].apply(lambda x: ' '.join([word for word in x.split() if word not in (sw)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stem = []\n",
    "for Claim in dat.ClaimDescription:\n",
    "    text = Claim.lower() # lowerlize\n",
    "    text = re.sub(r'[^a-zA-Z0-9]', ' ', text)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = text.split()  # plit\n",
    "    tokens=[stemmer.stem(i) for i in text if (i not in string.punctuation)&(i not in sw)]\n",
    "    Stem.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat['Stem'] = Stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['lift', 'tyre', 'injuri', 'right', 'arm', 'wrist', 'injuri'],\n",
       " ['step', 'around', 'crate', 'truck', 'tray', 'fractur', 'left', 'forearm'],\n",
       " ['cut', 'sharp', 'edg', 'cut', 'left', 'thumb'],\n",
       " ['dig', 'lower', 'back', 'lower', 'back', 'strain'],\n",
       " ['reach',\n",
       "  'shoulder',\n",
       "  'level',\n",
       "  'acut',\n",
       "  'muscl',\n",
       "  'strain',\n",
       "  'left',\n",
       "  'side',\n",
       "  'stomach']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Stem[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')\n",
    "# define lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lemm = []\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for Claim in dat.Stem:\n",
    "    #text = Claim.lower()\n",
    "    #text = re.sub(r'[^a-zA-Z0-9]', ' ', text)\n",
    "    #text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    #text = text.split()\n",
    "    tokens=[lemmatizer.lemmatize(i) for i in Claim] #if (i not in string.punctuation)&(i not in sw)]\n",
    "    Lemm.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['lift', 'tyre', 'injuri', 'right', 'arm', 'wrist', 'injuri'],\n",
       " ['step', 'around', 'crate', 'truck', 'tray', 'fractur', 'left', 'forearm'],\n",
       " ['cut', 'sharp', 'edg', 'cut', 'left', 'thumb'],\n",
       " ['dig', 'lower', 'back', 'lower', 'back', 'strain'],\n",
       " ['reach',\n",
       "  'shoulder',\n",
       "  'level',\n",
       "  'acut',\n",
       "  'muscl',\n",
       "  'strain',\n",
       "  'left',\n",
       "  'side',\n",
       "  'stomach']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lemm[0:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### N-grams\n",
    "\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for uni-gram\n",
    "def uni(tokens):\n",
    "    return tokens\n",
    "\n",
    "dat['Stem_uni'] = dat['Stem'].apply(lambda x: uni(x)) # uni-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [lift, tyre, injuri, right, arm, wrist, injuri]\n",
       "1    [step, around, crate, truck, tray, fractur, le...\n",
       "2                  [cut, sharp, edg, cut, left, thumb]\n",
       "3              [dig, lower, back, lower, back, strain]\n",
       "4    [reach, shoulder, level, acut, muscl, strain, ...\n",
       "Name: Stem_uni, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview the first 5 rows of the Stem_uni column\n",
    "dat['Stem_uni'].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. This is a continuation of Rlab #5. Consider data set \"WorkersComp.csv\". Use the unigram from Rlab #5 to build a topic model with 3 topics.\n",
    "\n",
    "a) Report the top 10 most probable words for each topic.\n",
    "\n",
    "b) Report the topic distribution for 3rd, 5th, and 9th claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.175*\"strain\" + 0.144*\"back\" + 0.088*\"lower\" + 0.077*\"lift\" + 0.051*\"shoulder\" + 0.032*\"neck\" + 0.025*\"injuri\" + 0.017*\"pain\" + 0.014*\"floor\" + 0.014*\"fell\"'), (1, '0.135*\"right\" + 0.064*\"eye\" + 0.058*\"left\" + 0.058*\"knee\" + 0.047*\"foreign\" + 0.046*\"bodi\" + 0.027*\"foot\" + 0.021*\"burn\" + 0.018*\"tendon\" + 0.016*\"wrist\"'), (2, '0.107*\"left\" + 0.078*\"right\" + 0.069*\"lacer\" + 0.058*\"finger\" + 0.055*\"hand\" + 0.049*\"struck\" + 0.026*\"cut\" + 0.026*\"fell\" + 0.024*\"thumb\" + 0.023*\"bruis\"')]\n"
     ]
    }
   ],
   "source": [
    "# Use the uni-gram to build a topic model with 3 topics\n",
    "from gensim import corpora, models\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(dat['Stem_uni'])\n",
    "\n",
    "# Create Corpus\n",
    "texts = dat['Stem_uni']\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = models.LdaModel(corpus=corpus,  # corpus\n",
    "                            id2word=id2word, # id to word\n",
    "                            num_topics=3,   # number of topics\n",
    "                            random_state=100, # random state\n",
    "                            chunksize=100, # chunk size\n",
    "                            passes=10, # number of passes\n",
    "                            alpha='auto', # auto alpha\n",
    "                            per_word_topics=True) # per word topic\n",
    "\n",
    "# Print the Keyword in the 3 topics\n",
    "print(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.175*\"strain\" + 0.144*\"back\" + 0.088*\"lower\" + 0.077*\"lift\" + 0.051*\"shoulder\" + 0.032*\"neck\" + 0.025*\"injuri\" + 0.017*\"pain\" + 0.014*\"floor\" + 0.014*\"fell\"'),\n",
       " (1,\n",
       "  '0.135*\"right\" + 0.064*\"eye\" + 0.058*\"left\" + 0.058*\"knee\" + 0.047*\"foreign\" + 0.046*\"bodi\" + 0.027*\"foot\" + 0.021*\"burn\" + 0.018*\"tendon\" + 0.016*\"wrist\"'),\n",
       " (2,\n",
       "  '0.107*\"left\" + 0.078*\"right\" + 0.069*\"lacer\" + 0.058*\"finger\" + 0.055*\"hand\" + 0.049*\"struck\" + 0.026*\"cut\" + 0.026*\"fell\" + 0.024*\"thumb\" + 0.023*\"bruis\"')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the top 10 most probable words for each of the 3 topics\n",
    "lda_model.show_topics(num_topics=3, num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.92666584), (1, 0.026386414), (2, 0.046947755)]\n",
      "[(0, 0.041167222), (1, 0.030991953), (2, 0.9278408)]\n",
      "[(0, 0.03149894), (1, 0.73781335), (2, 0.23068772)]\n"
     ]
    }
   ],
   "source": [
    "# Report the topic distribution for the 3rd, the 5th, and the 9th claim\n",
    "for i in [3,5,9]:\n",
    "    print(lda_model.get_document_topics(corpus[i]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Consider word embeedings from 'glove-twitter-25'.\n",
    "\n",
    "a) Report the embedding vectors for words \"tea\", \"phone\", \"coffee\", and \"moon\".\n",
    "\n",
    "b) Calculate the similarity matrix, and identify the pair with highest similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'corpora': ['semeval-2016-2017-task3-subtaskBC',\n",
       "  'semeval-2016-2017-task3-subtaskA-unannotated',\n",
       "  'patent-2017',\n",
       "  'quora-duplicate-questions',\n",
       "  'wiki-english-20171001',\n",
       "  'text8',\n",
       "  'fake-news',\n",
       "  '20-newsgroups',\n",
       "  '__testing_matrix-synopsis',\n",
       "  '__testing_multipart-matrix-synopsis'],\n",
       " 'models': ['fasttext-wiki-news-subwords-300',\n",
       "  'conceptnet-numberbatch-17-06-300',\n",
       "  'word2vec-ruscorpora-300',\n",
       "  'word2vec-google-news-300',\n",
       "  'glove-wiki-gigaword-50',\n",
       "  'glove-wiki-gigaword-100',\n",
       "  'glove-wiki-gigaword-200',\n",
       "  'glove-wiki-gigaword-300',\n",
       "  'glove-twitter-25',\n",
       "  'glove-twitter-50',\n",
       "  'glove-twitter-100',\n",
       "  'glove-twitter-200',\n",
       "  '__testing_word2vec-matrix-synopsis']}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "# information about the names of pre-trained models\n",
    "api.info(name_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "# Get pretrained 25 dimension embeddings from twitter data using glove\n",
    "model3 = api.load(\"glove-twitter-25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('coffee', 0.917603075504303),\n",
       " ('milk', 0.905985414981842),\n",
       " ('cream', 0.902299702167511),\n",
       " ('bread', 0.9010369777679443),\n",
       " ('ice', 0.8975890874862671),\n",
       " ('wine', 0.8962970972061157),\n",
       " ('food', 0.8928905725479126),\n",
       " ('soup', 0.8913701176643372),\n",
       " ('cake', 0.8854183554649353),\n",
       " ('cheese', 0.8838366270065308)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.most_similar(\"tea\")  # show words that similar to word 'tea'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text', 0.8994132876396179),\n",
       " ('id', 0.8980249762535095),\n",
       " ('screen', 0.8937858939170837),\n",
       " ('line', 0.8775232434272766),\n",
       " ('bc', 0.8747655153274536),\n",
       " ('call', 0.8741815090179443),\n",
       " ('phones', 0.8709824681282043),\n",
       " ('number', 0.8700169920921326),\n",
       " ('send', 0.8674761056900024),\n",
       " ('contacts', 0.8586106896400452)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.most_similar(\"phone\")  # show words that similar to word 'phone'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wine', 0.9737642407417297),\n",
       " ('food', 0.9210957884788513),\n",
       " ('bath', 0.9208705425262451),\n",
       " ('tea', 0.9176030158996582),\n",
       " ('cream', 0.9174261689186096),\n",
       " ('ice', 0.9166433215141296),\n",
       " ('beer', 0.9165878891944885),\n",
       " ('fresh', 0.9100037217140198),\n",
       " ('breakfast', 0.9091862440109253),\n",
       " ('drink', 0.9059524536132812)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.most_similar(\"coffee\")  # show words that similar to word 'coffee'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sun', 0.8894006609916687),\n",
       " ('ocean', 0.8843007683753967),\n",
       " ('soul', 0.8822776675224304),\n",
       " ('earth', 0.8816424012184143),\n",
       " ('shadow', 0.8800047636032104),\n",
       " ('shining', 0.8745366334915161),\n",
       " ('stone', 0.8728160262107849),\n",
       " ('light', 0.8710013628005981),\n",
       " ('heaven', 0.8636651039123535),\n",
       " ('joy', 0.8595615029335022)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.most_similar(\"moon\")  # show words that similar to word 'moon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.91760314\n",
      "0.506179\n",
      "0.56842726\n",
      "0.5559021\n",
      "0.65578926\n",
      "0.50199217\n"
     ]
    }
   ],
   "source": [
    "# Calculate the similarity between the four words, and identify the most similar pair\n",
    "print(model3.similarity(\"tea\", \"coffee\"))\n",
    "print(model3.similarity(\"tea\", \"phone\"))\n",
    "print(model3.similarity(\"tea\", \"moon\"))\n",
    "print(model3.similarity(\"coffee\", \"phone\"))\n",
    "print(model3.similarity(\"coffee\", \"moon\"))\n",
    "print(model3.similarity(\"phone\", \"moon\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The most similar pair is 'tea' and 'coffee'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
