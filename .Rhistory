pval_sims # display the p-value for this first simulation (note this is just repeating what we saw in t-test from line 20)
# Now let's use bootstrapping to record the results of many simulations.
for (i in 1:4999){  # loop through 1,999 more times to get 5k total simulations
tempcontrol <- runif(n) #create n random uniform draws for control for this simulation
success_control <- as.integer(tempcontrol < p_control) # creates binary indicator for success with p_control chance
temptreat <- runif(n) #create n random uniform draws for treat
success_treat <- as.integer(temptreat < p_treat) # creates binary indicator for success with p_treat chance
results_t <- t.test(success_treat, success_control, alternative = "greater") #Record the t.test of equality in the two for this round of simulations
pval_sims <-  c(pval_sims, results_t$p.value) #update our vector of pvalues to add the value from this simulation round
ate_sims <-  c(ate_sims, mean(success_treat)-mean(success_control)) #update the vector of ATE estimates to include this simulation
}
# Let's look at the first 6 simulation results for both the ate and the pval
head(ate_sims, 6)  # head(vector, x) gives you the first x instances of the vector
head(pval_sims, 6)
# Now we explore the results of the 2k simulations we ran.
# Look at a histogram of all the 2k examples of the treatment effect
#  Should be centered around the value of the difference we initiated in lines 5 & 6
hist(ate_sims,
main = "Distribution of ATEs",
xlab = "ATE")
# Calculate the average across the 2k treatment effects (should be close to the difference from lines 5 & 6)
mean <- mean(ate_sims)
# What fraction of the 5k example p-values were below 0.05 (meaning we detect stat sig difference)?
mean(pval_sims < 0.05)
# Suppose group A is our control with true proportion = 0.25 and our treatment will increase it to 0.26.
# If we run tests where we select treatment over control when the p-value is less than 0.05...
# What will the power be when N = 3500 (n = 1750 per cell)? (That is what share of time will we detect
# the true difference as statistically significant?
power.prop.test(n = n, p1 = p_control, p2 = p_treat, sig.level = 0.05, power = NULL, alternative ="one.sided", strict = FALSE)
# Notice you should be seeing about the same numbers when we compare the power calculator and the simulation results here
# What if we only act on the treatment effect and declare it a "winner" when our test
#   has a p-values <= .05?  What is the distribution of the treatment effects we will see?
hist(ate_sims[ate_sims > 0 & pval_sims<0.05],
main = "Distribution of ATE when treatment is stat sig",
xlab = "Statistically significant ATE",
xlim = c(0,0.10)) #the [] part tells it "conditional on positive ate and pval_sims being <= .05
# What is the average level of the ate when we find a significant effect?
mean(ate_sims[ate_sims > 0 & pval_sims<0.05])
# Suppose we will continue to declare the treatment the "winner" and implement it when we get a p-value < 0.05
# But now we want to figure out the sample size we would need so that we will correctly detect the treatment
# as the winner when it has a 0.26 effect relative to 0.25 for control.  What is that sample size we need?
power.prop.test(n = NULL, p1 = 0.2, p2 = 0.21, sig.level = 0.05, power = 0.80, alternative ="one.sided", strict = FALSE)
# Suppose we wanted to use the sample size recommended for "Got for It" problems in the article by Matt Gershoff
# If we still have an MDE of 0.01 and control level of 0.25, what do we need for sample size to have 80% power to
# detect the MDE when we select the treatment whenever it has higher estimated treatment effect than control?
power.prop.test(n = NULL, p1 = p_control, p2 = p_treat, sig.level = 0.5, power = 0.80, alternative ="one.sided", strict = FALSE)
# And what if we want power to be 90% in that case?
power.prop.test(n = NULL, p1 = p_control, p2 = p_treat, sig.level = 0.5, power = 0.90, alternative ="one.sided", strict = FALSE)
mean
rm(list = ls()) # Clear the workspace
N <- 3500  #The total sample size across treat & control.
n <- N/2   #Sample size for each group (1/2 the total)
p_control <- .2 # underlying probability of success in control
p_treat <- .2 # underlying probability of success in treat
tempcontrol <- runif(n) #create n random uniform draws for control
success_control <- as.integer(tempcontrol < p_control) # creates binary indicator for success with p_control chance
temptreat <- rnorm(n, mean = 0, sd = 0.01) #create n random uniform draws for treat
success_treat <- as.integer(temptreat < p_treat) # creates binary indicator for success with p_treat chance
mean(success_control) # let's look at the mean for success in control in our sample
mean(success_treat) # let's look at the mean for success in treat in our sample
mean <- mean(success_treat)-mean(success_control) #Estimated treatment effect
results_t<-t.test(success_treat, success_control, alternative = "greater") #Record the t.test of equality in the two.
#The t-test will give you the p-value on the treatment effect being different between treatment and control.
results_t #display the t-test results
pval_sims <-  results_t$p.value  #record the p-value from the t-test for this first simulation.
ate_sims <- mean(success_treat)-mean(success_control) #record the ATE from this first simulation
ate_sims #display the treatment effect from this first simulation
pval_sims # display the p-value for this first simulation (note this is just repeating what we saw in t-test from line 20)
# Now let's use bootstrapping to record the results of many simulations.
for (i in 1:4999){  # loop through 1,999 more times to get 5k total simulations
tempcontrol <- runif(n) #create n random uniform draws for control for this simulation
success_control <- as.integer(tempcontrol < p_control) # creates binary indicator for success with p_control chance
temptreat <- runif(n) #create n random uniform draws for treat
success_treat <- as.integer(temptreat < p_treat) # creates binary indicator for success with p_treat chance
results_t <- t.test(success_treat, success_control, alternative = "greater") #Record the t.test of equality in the two for this round of simulations
pval_sims <-  c(pval_sims, results_t$p.value) #update our vector of pvalues to add the value from this simulation round
ate_sims <-  c(ate_sims, mean(success_treat)-mean(success_control)) #update the vector of ATE estimates to include this simulation
}
# Let's look at the first 6 simulation results for both the ate and the pval
head(ate_sims, 6)  # head(vector, x) gives you the first x instances of the vector
head(pval_sims, 6)
# Now we explore the results of the 2k simulations we ran.
# Look at a histogram of all the 2k examples of the treatment effect
#  Should be centered around the value of the difference we initiated in lines 5 & 6
hist(ate_sims,
main = "Distribution of ATEs",
xlab = "ATE")
# Calculate the average across the 2k treatment effects (should be close to the difference from lines 5 & 6)
mean <- mean(ate_sims)
# What fraction of the 5k example p-values were below 0.05 (meaning we detect stat sig difference)?
mean(pval_sims < 0.05)
# Suppose group A is our control with true proportion = 0.25 and our treatment will increase it to 0.26.
# If we run tests where we select treatment over control when the p-value is less than 0.05...
# What will the power be when N = 3500 (n = 1750 per cell)? (That is what share of time will we detect
# the true difference as statistically significant?
power.prop.test(n = n, p1 = p_control, p2 = p_treat, sig.level = 0.05, power = NULL, alternative ="one.sided", strict = FALSE)
# Notice you should be seeing about the same numbers when we compare the power calculator and the simulation results here
# What if we only act on the treatment effect and declare it a "winner" when our test
#   has a p-values <= .05?  What is the distribution of the treatment effects we will see?
hist(ate_sims[ate_sims > 0 & pval_sims<0.05],
main = "Distribution of ATE when treatment is stat sig",
xlab = "Statistically significant ATE",
xlim = c(0,0.10)) #the [] part tells it "conditional on positive ate and pval_sims being <= .05
# What is the average level of the ate when we find a significant effect?
mean(ate_sims[ate_sims > 0 & pval_sims<0.05])
# Suppose we will continue to declare the treatment the "winner" and implement it when we get a p-value < 0.05
# But now we want to figure out the sample size we would need so that we will correctly detect the treatment
# as the winner when it has a 0.26 effect relative to 0.25 for control.  What is that sample size we need?
power.prop.test(n = NULL, p1 = 0.2, p2 = 0.21, sig.level = 0.05, power = 0.80, alternative ="one.sided", strict = FALSE)
# Suppose we wanted to use the sample size recommended for "Got for It" problems in the article by Matt Gershoff
# If we still have an MDE of 0.01 and control level of 0.25, what do we need for sample size to have 80% power to
# detect the MDE when we select the treatment whenever it has higher estimated treatment effect than control?
power.prop.test(n = NULL, p1 = p_control, p2 = p_treat, sig.level = 0.5, power = 0.80, alternative ="one.sided", strict = FALSE)
mean
rm(list = ls()) # Clear the workspace
N <- 3500  #The total sample size across treat & control.
n <- N/2   #Sample size for each group (1/2 the total)
p_control <- .2 # underlying probability of success in control
p_treat <- .2 # underlying probability of success in treat
tempcontrol <- runif(n) #create n random uniform draws for control
success_control <- as.integer(tempcontrol < p_control) # creates binary indicator for success with p_control chance
temptreat <- rnorm(n, mean = 0, sd = 0.01) #create n random uniform draws for treat
success_treat <- as.integer(temptreat < p_treat) # creates binary indicator for success with p_treat chance
mean(success_control) # let's look at the mean for success in control in our sample
mean(success_treat) # let's look at the mean for success in treat in our sample
mean <- mean(success_treat)-mean(success_control) #Estimated treatment effect
results_t<-t.test(success_treat, success_control, alternative = "greater") #Record the t.test of equality in the two.
#The t-test will give you the p-value on the treatment effect being different between treatment and control.
results_t #display the t-test results
pval_sims <-  results_t$p.value  #record the p-value from the t-test for this first simulation.
ate_sims <- mean(success_treat)-mean(success_control) #record the ATE from this first simulation
ate_sims #display the treatment effect from this first simulation
pval_sims # display the p-value for this first simulation (note this is just repeating what we saw in t-test from line 20)
# Now let's use bootstrapping to record the results of many simulations.
for (i in 1:4999){  # loop through 1,999 more times to get 5k total simulations
tempcontrol <- runif(n) #create n random uniform draws for control for this simulation
success_control <- as.integer(tempcontrol < p_control) # creates binary indicator for success with p_control chance
temptreat <- runif(n) #create n random uniform draws for treat
success_treat <- as.integer(temptreat < p_treat) # creates binary indicator for success with p_treat chance
results_t <- t.test(success_treat, success_control, alternative = "greater") #Record the t.test of equality in the two for this round of simulations
pval_sims <-  c(pval_sims, results_t$p.value) #update our vector of pvalues to add the value from this simulation round
ate_sims <-  c(ate_sims, mean(success_treat)-mean(success_control)) #update the vector of ATE estimates to include this simulation
}
# Let's look at the first 6 simulation results for both the ate and the pval
head(ate_sims, 6)  # head(vector, x) gives you the first x instances of the vector
head(pval_sims, 6)
# Now we explore the results of the 2k simulations we ran.
# Look at a histogram of all the 2k examples of the treatment effect
#  Should be centered around the value of the difference we initiated in lines 5 & 6
hist(ate_sims,
main = "Distribution of ATEs",
xlab = "ATE")
# Calculate the average across the 2k treatment effects (should be close to the difference from lines 5 & 6)
mean <- mean(ate_sims)
mean
# What fraction of the 5k example p-values were below 0.05 (meaning we detect stat sig difference)?
mean(pval_sims < 0.05)
# Suppose group A is our control with true proportion = 0.25 and our treatment will increase it to 0.26.
# If we run tests where we select treatment over control when the p-value is less than 0.05...
# What will the power be when N = 3500 (n = 1750 per cell)? (That is what share of time will we detect
# the true difference as statistically significant?
power.prop.test(n = n, p1 = p_control, p2 = p_treat, sig.level = 0.05, power = NULL, alternative ="one.sided", strict = FALSE)
# Notice you should be seeing about the same numbers when we compare the power calculator and the simulation results here
# What if we only act on the treatment effect and declare it a "winner" when our test
#   has a p-values <= .05?  What is the distribution of the treatment effects we will see?
hist(ate_sims[ate_sims > 0 & pval_sims<0.05],
main = "Distribution of ATE when treatment is stat sig",
xlab = "Statistically significant ATE",
xlim = c(0,0.10)) #the [] part tells it "conditional on positive ate and pval_sims being <= .05
# What is the average level of the ate when we find a significant effect?
mean(ate_sims[ate_sims > 0 & pval_sims<0.05])
# Suppose we will continue to declare the treatment the "winner" and implement it when we get a p-value < 0.05
# But now we want to figure out the sample size we would need so that we will correctly detect the treatment
# as the winner when it has a 0.26 effect relative to 0.25 for control.  What is that sample size we need?
power.prop.test(n = NULL, p1 = 0.2, p2 = 0.21, sig.level = 0.05, power = 0.80, alternative ="one.sided", strict = FALSE)
# Suppose we wanted to use the sample size recommended for "Got for It" problems in the article by Matt Gershoff
# If we still have an MDE of 0.01 and control level of 0.25, what do we need for sample size to have 80% power to
# detect the MDE when we select the treatment whenever it has higher estimated treatment effect than control?
power.prop.test(n = NULL, p1 = p_control, p2 = p_treat, sig.level = 0.5, power = 0.80, alternative ="one.sided", strict = FALSE)
hist(success_treat,
main = "Distribution of treatment,
xlab = "Statistically significant ATE",
hist(success_treat,
main = "Distribution of treatment",
xlab = "Statistically significant ATE",
xlim = c(0,0.10))
hist(success_treat,
main = "Distribution of treatment",
xlab = "Statistically significant ATE",
xlim = )
hist(success_treat)
hist(success_treat)
rm(list = ls()) # Clear the workspace
p_control <- .2 # underlying probability of success in control
p_treat <- .2 + rnorm(n, mean = 0, sd = 0.01) # underlying probability of success in treat
rm(list = ls()) # Clear the workspace
N <- 3500  #The total sample size across treat & control.
n <- N/2   #Sample size for each group (1/2 the total)
p_control <- .2 # underlying probability of success in control
p_treat <- .2 + rnorm(n, mean = 0, sd = 0.01) # underlying probability of success in treat
rm(list = ls()) # Clear the workspace
N <- 3500  #The total sample size across treat & control.
n <- N/2   #Sample size for each group (1/2 the total)
p_control <- .2 # underlying probability of success in control
p_treat <- .2 + rnorm(n, mean = 0, sd = 0.01) # underlying probability of success in treat
tempcontrol <- runif(n) #create n random uniform draws for control
success_control <- runif(n) # creates binary indicator for success with p_control chance
temptreat <- rnorm(n, mean = 0, sd = 0.01) #create n random uniform draws for treat
success_treat <- as.integer(temptreat < p_treat) # creates binary indicator for success with p_treat chance
mean(success_control) # let's look at the mean for success in control in our sample
mean(success_treat) # let's look at the mean for success in treat in our sample
mean <- mean(success_treat)-mean(success_control) #Estimated treatment effect
results_t<-t.test(success_treat, success_control, alternative = "greater") #Record the t.test of equality in the two.
#The t-test will give you the p-value on the treatment effect being different between treatment and control.
results_t #display the t-test results
pval_sims <-  results_t$p.value  #record the p-value from the t-test for this first simulation.
ate_sims <- mean(success_treat)-mean(success_control) #record the ATE from this first simulation
ate_sims #display the treatment effect from this first simulation
pval_sims # display the p-value for this first simulation (note this is just repeating what we saw in t-test from line 20)
# Now let's use bootstrapping to record the results of many simulations.
for (i in 1:4999){  # loop through 1,999 more times to get 5k total simulations
tempcontrol <- runif(n) #create n random uniform draws for control for this simulation
success_control <- as.integer(tempcontrol < p_control) # creates binary indicator for success with p_control chance
temptreat <- runif(n) #create n random uniform draws for treat
success_treat <- as.integer(temptreat < p_treat) # creates binary indicator for success with p_treat chance
results_t <- t.test(success_treat, success_control, alternative = "greater") #Record the t.test of equality in the two for this round of simulations
pval_sims <-  c(pval_sims, results_t$p.value) #update our vector of pvalues to add the value from this simulation round
ate_sims <-  c(ate_sims, mean(success_treat)-mean(success_control)) #update the vector of ATE estimates to include this simulation
}
# Let's look at the first 6 simulation results for both the ate and the pval
head(ate_sims, 6)  # head(vector, x) gives you the first x instances of the vector
head(pval_sims, 6)
# Now we explore the results of the 2k simulations we ran.
# Look at a histogram of all the 2k examples of the treatment effect
#  Should be centered around the value of the difference we initiated in lines 5 & 6
hist(ate_sims,
main = "Distribution of ATEs",
xlab = "ATE")
# Calculate the average across the 2k treatment effects (should be close to the difference from lines 5 & 6)
mean <- mean(ate_sims)
mean
# What fraction of the 5k example p-values were below 0.05 (meaning we detect stat sig difference)?
mean(pval_sims < 0.05)
# Suppose group A is our control with true proportion = 0.25 and our treatment will increase it to 0.26.
# If we run tests where we select treatment over control when the p-value is less than 0.05...
# What will the power be when N = 3500 (n = 1750 per cell)? (That is what share of time will we detect
# the true difference as statistically significant?
power.prop.test(n = n, p1 = p_control, p2 = p_treat, sig.level = 0.05, power = NULL, alternative ="one.sided", strict = FALSE)
# Notice you should be seeing about the same numbers when we compare the power calculator and the simulation results here
# What if we only act on the treatment effect and declare it a "winner" when our test
#   has a p-values <= .05?  What is the distribution of the treatment effects we will see?
hist(ate_sims[ate_sims > 0 & pval_sims<0.05],
main = "Distribution of ATE when treatment is stat sig",
xlab = "Statistically significant ATE",
xlim = c(0,0.10)) #the [] part tells it "conditional on positive ate and pval_sims being <= .05
# What is the average level of the ate when we find a significant effect?
mean(ate_sims[ate_sims > 0 & pval_sims<0.05])
# Suppose we will continue to declare the treatment the "winner" and implement it when we get a p-value < 0.05
# But now we want to figure out the sample size we would need so that we will correctly detect the treatment
# as the winner when it has a 0.26 effect relative to 0.25 for control.  What is that sample size we need?
power.prop.test(n = NULL, p1 = 0.2, p2 = 0.21, sig.level = 0.05, power = 0.80, alternative ="one.sided", strict = FALSE)
# Suppose we wanted to use the sample size recommended for "Got for It" problems in the article by Matt Gershoff
# If we still have an MDE of 0.01 and control level of 0.25, what do we need for sample size to have 80% power to
# detect the MDE when we select the treatment whenever it has higher estimated treatment effect than control?
power.prop.test(n = NULL, p1 = p_control, p2 = p_treat, sig.level = 0.5, power = 0.80, alternative ="one.sided", strict = FALSE)
hist(p_treat)
rm(list = ls()) # Clear the workspace
N <- 40302  #The total sample size across treat & control.
n <- N/2   #Sample size for each group (1/2 the total)
p_control <- .2 # underlying probability of success in control
p_treat <- .2 + rnorm(n, mean = 0, sd = 0.01) # underlying probability of success in treat
tempcontrol <- runif(n) #create n random uniform draws for control
success_control <- runif(n) # creates binary indicator for success with p_control chance
temptreat <- rnorm(n, mean = 0, sd = 0.01) #create n random uniform draws for treat
success_treat <- as.integer(temptreat < p_treat) # creates binary indicator for success with p_treat chance
mean(success_control) # let's look at the mean for success in control in our sample
mean(success_treat) # let's look at the mean for success in treat in our sample
mean <- mean(success_treat)-mean(success_control) #Estimated treatment effect
results_t<-t.test(success_treat, success_control, alternative = "greater") #Record the t.test of equality in the two.
#The t-test will give you the p-value on the treatment effect being different between treatment and control.
results_t #display the t-test results
pval_sims <-  results_t$p.value  #record the p-value from the t-test for this first simulation.
ate_sims <- mean(success_treat)-mean(success_control) #record the ATE from this first simulation
ate_sims #display the treatment effect from this first simulation
pval_sims # display the p-value for this first simulation (note this is just repeating what we saw in t-test from line 20)
# Now let's use bootstrapping to record the results of many simulations.
for (i in 1:4999){  # loop through 1,999 more times to get 5k total simulations
tempcontrol <- runif(n) #create n random uniform draws for control for this simulation
success_control <- as.integer(tempcontrol < p_control) # creates binary indicator for success with p_control chance
temptreat <- runif(n) #create n random uniform draws for treat
success_treat <- as.integer(temptreat < p_treat) # creates binary indicator for success with p_treat chance
results_t <- t.test(success_treat, success_control, alternative = "greater") #Record the t.test of equality in the two for this round of simulations
pval_sims <-  c(pval_sims, results_t$p.value) #update our vector of pvalues to add the value from this simulation round
ate_sims <-  c(ate_sims, mean(success_treat)-mean(success_control)) #update the vector of ATE estimates to include this simulation
}
# Let's look at the first 6 simulation results for both the ate and the pval
head(ate_sims, 6)  # head(vector, x) gives you the first x instances of the vector
head(pval_sims, 6)
# Now we explore the results of the 2k simulations we ran.
# Look at a histogram of all the 2k examples of the treatment effect
#  Should be centered around the value of the difference we initiated in lines 5 & 6
hist(ate_sims,
main = "Distribution of ATEs",
xlab = "ATE")
# Calculate the average across the 2k treatment effects (should be close to the difference from lines 5 & 6)
mean <- mean(ate_sims)
mean
# What fraction of the 5k example p-values were below 0.05 (meaning we detect stat sig difference)?
mean(pval_sims < 0.05)
# Suppose group A is our control with true proportion = 0.25 and our treatment will increase it to 0.26.
# If we run tests where we select treatment over control when the p-value is less than 0.05...
# What will the power be when N = 3500 (n = 1750 per cell)? (That is what share of time will we detect
# the true difference as statistically significant?
power.prop.test(n = n, p1 = p_control, p2 = p_treat, sig.level = 0.05, power = NULL, alternative ="one.sided", strict = FALSE)
# Notice you should be seeing about the same numbers when we compare the power calculator and the simulation results here
# What if we only act on the treatment effect and declare it a "winner" when our test
#   has a p-values <= .05?  What is the distribution of the treatment effects we will see?
hist(ate_sims[ate_sims > 0 & pval_sims<0.05],
main = "Distribution of ATE when treatment is stat sig",
xlab = "Statistically significant ATE",
xlim = c(0,0.10)) #the [] part tells it "conditional on positive ate and pval_sims being <= .05
# What is the average level of the ate when we find a significant effect?
mean(ate_sims[ate_sims > 0 & pval_sims<0.05])
# Suppose we will continue to declare the treatment the "winner" and implement it when we get a p-value < 0.05
# But now we want to figure out the sample size we would need so that we will correctly detect the treatment
# as the winner when it has a 0.26 effect relative to 0.25 for control.  What is that sample size we need?
power.prop.test(n = NULL, p1 = 0.2, p2 = 0.21, sig.level = 0.05, power = 0.80, alternative ="one.sided", strict = FALSE)
# Suppose we wanted to use the sample size recommended for "Got for It" problems in the article by Matt Gershoff
# If we still have an MDE of 0.01 and control level of 0.25, what do we need for sample size to have 80% power to
# detect the MDE when we select the treatment whenever it has higher estimated treatment effect than control?
power.prop.test(n = NULL, p1 = p_control, p2 = p_treat, sig.level = 0.5, power = 0.80, alternative ="one.sided", strict = FALSE)
hist(p_treat)
rm(list = ls()) # Clear the workspace
N <- 40302  #The total sample size across treat & control.
n <- N/2   #Sample size for each group (1/2 the total)
p_control <- .2 # underlying probability of success in control
p_treat <- .2 + rnorm(n, mean = 0, sd = 0.01) # underlying probability of success in treat
tempcontrol <- runif(n) #create n random uniform draws for control
success_control <- runif(n) # creates binary indicator for success with p_control chance
temptreat <- runif(n) #create n random uniform draws for treat
success_treat <- as.integer(temptreat < p_treat) # creates binary indicator for success with p_treat chance
mean(success_control) # let's look at the mean for success in control in our sample
mean(success_treat) # let's look at the mean for success in treat in our sample
mean <- mean(success_treat)-mean(success_control) #Estimated treatment effect
results_t<-t.test(success_treat, success_control, alternative = "greater") #Record the t.test of equality in the two.
#The t-test will give you the p-value on the treatment effect being different between treatment and control.
results_t #display the t-test results
pval_sims <-  results_t$p.value  #record the p-value from the t-test for this first simulation.
ate_sims <- mean(success_treat)-mean(success_control) #record the ATE from this first simulation
ate_sims #display the treatment effect from this first simulation
pval_sims # display the p-value for this first simulation (note this is just repeating what we saw in t-test from line 20)
# Now let's use bootstrapping to record the results of many simulations.
for (i in 1:4999){  # loop through 1,999 more times to get 5k total simulations
tempcontrol <- runif(n) #create n random uniform draws for control for this simulation
success_control <- as.integer(tempcontrol < p_control) # creates binary indicator for success with p_control chance
temptreat <- runif(n) #create n random uniform draws for treat
success_treat <- as.integer(temptreat < p_treat) # creates binary indicator for success with p_treat chance
results_t <- t.test(success_treat, success_control, alternative = "greater") #Record the t.test of equality in the two for this round of simulations
pval_sims <-  c(pval_sims, results_t$p.value) #update our vector of pvalues to add the value from this simulation round
ate_sims <-  c(ate_sims, mean(success_treat)-mean(success_control)) #update the vector of ATE estimates to include this simulation
}
# Let's look at the first 6 simulation results for both the ate and the pval
head(ate_sims, 6)  # head(vector, x) gives you the first x instances of the vector
head(pval_sims, 6)
# Now we explore the results of the 2k simulations we ran.
# Look at a histogram of all the 2k examples of the treatment effect
#  Should be centered around the value of the difference we initiated in lines 5 & 6
hist(ate_sims,
main = "Distribution of ATEs",
xlab = "ATE")
# Calculate the average across the 2k treatment effects (should be close to the difference from lines 5 & 6)
mean <- mean(ate_sims)
mean
# What fraction of the 5k example p-values were below 0.05 (meaning we detect stat sig difference)?
mean(pval_sims < 0.05)
# Suppose group A is our control with true proportion = 0.25 and our treatment will increase it to 0.26.
# If we run tests where we select treatment over control when the p-value is less than 0.05...
# What will the power be when N = 3500 (n = 1750 per cell)? (That is what share of time will we detect
# the true difference as statistically significant?
power.prop.test(n = n, p1 = p_control, p2 = p_treat, sig.level = 0.05, power = NULL, alternative ="one.sided", strict = FALSE)
# Notice you should be seeing about the same numbers when we compare the power calculator and the simulation results here
# What if we only act on the treatment effect and declare it a "winner" when our test
#   has a p-values <= .05?  What is the distribution of the treatment effects we will see?
hist(ate_sims[ate_sims > 0 & pval_sims<0.05],
main = "Distribution of ATE when treatment is stat sig",
xlab = "Statistically significant ATE",
xlim = c(0,0.10)) #the [] part tells it "conditional on positive ate and pval_sims being <= .05
# What is the average level of the ate when we find a significant effect?
mean(ate_sims[ate_sims > 0 & pval_sims<0.05])
# Suppose we will continue to declare the treatment the "winner" and implement it when we get a p-value < 0.05
# But now we want to figure out the sample size we would need so that we will correctly detect the treatment
# as the winner when it has a 0.26 effect relative to 0.25 for control.  What is that sample size we need?
power.prop.test(n = NULL, p1 = 0.2, p2 = 0.21, sig.level = 0.05, power = 0.80, alternative ="one.sided", strict = FALSE)
# Suppose we wanted to use the sample size recommended for "Got for It" problems in the article by Matt Gershoff
# If we still have an MDE of 0.01 and control level of 0.25, what do we need for sample size to have 80% power to
# detect the MDE when we select the treatment whenever it has higher estimated treatment effect than control?
power.prop.test(n = NULL, p1 = p_control, p2 = p_treat, sig.level = 0.5, power = 0.80, alternative ="one.sided", strict = FALSE)
hist(p_treat)
rm(list = ls()) # Clear the workspace
N <- 40302  #The total sample size across treat & control.
n <- N/2   #Sample size for each group (1/2 the total)
p_control <- .2 # underlying probability of success in control
p_treat <- .2 + rnorm(n, mean = 0, sd = 0.01) # underlying probability of success in treat
tempcontrol <- runif(n) #create n random uniform draws for control
success_control <- runif(n) # creates binary indicator for success with p_control chance
temptreat <- runif(n) #create n random uniform draws for treat
success_treat <- as.integer(temptreat < p_treat) # creates binary indicator for success with p_treat chance
mean(success_control) # let's look at the mean for success in control in our sample
mean(success_treat) # let's look at the mean for success in treat in our sample
mean <- mean(success_treat)-mean(success_control) #Estimated treatment effect
results_t<-t.test(success_treat, success_control, alternative = "greater") #Record the t.test of equality in the two.
#The t-test will give you the p-value on the treatment effect being different between treatment and control.
results_t #display the t-test results
pval_sims <-  results_t$p.value  #record the p-value from the t-test for this first simulation.
ate_sims <- mean(success_treat)-mean(success_control) #record the ATE from this first simulation
ate_sims #display the treatment effect from this first simulation
pval_sims # display the p-value for this first simulation (note this is just repeating what we saw in t-test from line 20)
# Now let's use bootstrapping to record the results of many simulations.
for (i in 1:4999){  # loop through 1,999 more times to get 5k total simulations
tempcontrol <- runif(n) #create n random uniform draws for control for this simulation
success_control <- as.integer(tempcontrol < p_control) # creates binary indicator for success with p_control chance
temptreat <- runif(n) #create n random uniform draws for treat
success_treat <- as.integer(temptreat < p_treat) # creates binary indicator for success with p_treat chance
results_t <- t.test(success_treat, success_control, alternative = "greater") #Record the t.test of equality in the two for this round of simulations
pval_sims <-  c(pval_sims, results_t$p.value) #update our vector of pvalues to add the value from this simulation round
ate_sims <-  c(ate_sims, mean(success_treat)-mean(success_control)) #update the vector of ATE estimates to include this simulation
}
# Let's look at the first 6 simulation results for both the ate and the pval
head(ate_sims, 6)  # head(vector, x) gives you the first x instances of the vector
head(pval_sims, 6)
# Now we explore the results of the 2k simulations we ran.
# Look at a histogram of all the 2k examples of the treatment effect
#  Should be centered around the value of the difference we initiated in lines 5 & 6
hist(ate_sims,
main = "Distribution of ATEs",
xlab = "ATE")
# Calculate the average across the 2k treatment effects (should be close to the difference from lines 5 & 6)
mean <- mean(ate_sims)
mean
# What fraction of the 5k example p-values were below 0.05 (meaning we detect stat sig difference)?
mean(pval_sims < 0.5)
# Suppose group A is our control with true proportion = 0.25 and our treatment will increase it to 0.26.
# If we run tests where we select treatment over control when the p-value is less than 0.05...
# What will the power be when N = 3500 (n = 1750 per cell)? (That is what share of time will we detect
# the true difference as statistically significant?
power.prop.test(n = n, p1 = p_control, p2 = p_treat, sig.level = 0.5, power = NULL, alternative ="one.sided", strict = FALSE)
# Notice you should be seeing about the same numbers when we compare the power calculator and the simulation results here
# What if we only act on the treatment effect and declare it a "winner" when our test
#   has a p-values <= .05?  What is the distribution of the treatment effects we will see?
hist(ate_sims[ate_sims > 0 & pval_sims<0.5],
main = "Distribution of ATE when treatment is stat sig",
xlab = "Statistically significant ATE",
xlim = c(0,0.10)) #the [] part tells it "conditional on positive ate and pval_sims being <= .05
# What is the average level of the ate when we find a significant effect?
mean(ate_sims[ate_sims > 0 & pval_sims<0.5])
# Suppose we will continue to declare the treatment the "winner" and implement it when we get a p-value < 0.05
# But now we want to figure out the sample size we would need so that we will correctly detect the treatment
# as the winner when it has a 0.26 effect relative to 0.25 for control.  What is that sample size we need?
power.prop.test(n = NULL, p1 = 0.2, p2 = 0.21, sig.level = 0.5, power = 0.95, alternative ="one.sided", strict = FALSE)
# Suppose we wanted to use the sample size recommended for "Got for It" problems in the article by Matt Gershoff
# If we still have an MDE of 0.01 and control level of 0.25, what do we need for sample size to have 80% power to
# detect the MDE when we select the treatment whenever it has higher estimated treatment effect than control?
power.prop.test(n = NULL, p1 = p_control, p2 = p_treat, sig.level = 0.5, power = 0.95, alternative ="one.sided", strict = FALSE)
hist(p_treat)
install.packages("rmarkdown")
install.packages("rmarkdown")
setwd("~/GitHub/ACTSCI657")
PropertyFund.txt
PropertyFund.txt
read.delim("PropertyFund.txt", header = TRUE, sep = "\t", dec = ".", stringsAsFactors = FALSE) -> PropertyFund
View(PropertyFund)
View(PropertyFund)
View(PropertyFund)
read.table("PropertyFund.txt", header = TRUE, sep = "\t", dec = ".", stringsAsFactors = FALSE) -> PropertyFund
View(PropertyFund)
View(PropertyFund)
read.table("PropertyFund.txt", header = TRUE, sep = " ", dec = ".", stringsAsFactors = FALSE) -> PropertyFund
View(PropertyFund)
View(PropertyFund)
install.packages("languageserver")
knitr::opts_chunk$set(echo = TRUE)
read.table("PropertyFund.txt", header = TRUE, sep = " ", dec = ".", stringsAsFactors = FALSE) -> PropertyFund
# Define Variable
table(PropertyFund$Freq) -> freq_tab
# Mean of Freq
mean(PropertyFund$Freq) -> mean_freq
mean_freq
# Median of Freq
median(PropertyFund$Freq) -> median_freq
median_freq
# Bar plot
barplot(freq_tab, main = "Frequency of Freq", xlab = "Freq", ylab = "Frequency")
# Split data into two parts
PropertyFund$Year <- as.numeric(substr(PropertyFund$Date, 1, 4))
# Training data
PropertyFund[PropertyFund$Year <= 2008, ] -> PropertyFund_train
# Test data
PropertyFund[PropertyFund$Year > 2008, ] -> PropertyFund_test
# Fit a simple linear regression of Freq on Coverage using training data
lm(Freq ~ Coverage, data = PropertyFund_train) -> model1
# Report results
summary(model1)
# Refit the model by including EntityType in addition to Coverage
lm(Freq ~ Coverage + EntityType, data = PropertyFund_train) -> model2
View(model2)
# Report the results
summary(model2)
# Use the fitted model in Question 5 to make predictions of Freq in the test data
predict(model2, PropertyFund_test) -> predicted_freq
# Calculate the linear correlation coefficient between predicted values and observed values
cor(predicted_freq, PropertyFund_test$Freq) -> corr
corr
